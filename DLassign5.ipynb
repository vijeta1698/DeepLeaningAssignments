{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Why would you want to use the Data API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs are needed to bring applications together in order to perform a designed function built around sharing data and executing pre-defined processes. They work as the middle man, allowing developers to build new programmatic interactions between the various applications people and businesses use on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What are the benefits of splitting a large dataset into multiple files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Multiple Users can Access Data Simultaneously\n",
    "When the data in a database is split, in frontend and backend, it can be easily supplied to multiple users sharing a network. If the backend is stored on file server and frontend on workstations, multiple users will be able to access data and make changes wherever necessary. If any of the user makes any kind of change in the databases, all authorized users will instantly get updates regarding the changes made.\n",
    "\n",
    "2. Provides Better Protection\n",
    "By splitting your databases, you can make them more secure, if all your database designs are stored in the backend, no one from the front end will be able to make any kind of changes in the tables. And those accessing the backend will not be able to view the interface objects. Thus by splitting databases, you can limit the access of users and protect your databases.\n",
    "\n",
    "3. Allows for Future Planning\n",
    "\n",
    "You can reduce the size of an ever growing database by splitting it. After splitting the database, you can also upsize split databases to larger relational database management software like SQL Server. This is possible due to the easily formed links between frontend and SQL tables. Thus by using splitting in databases, the organization can use SQL for front end and Access for backend.\n",
    "\n",
    "4. Easy to Modify User Interface\n",
    "Databases need to be modified with growing business requirements. You need to update them with new features, most of which are possible only at the frontend, usually in the form of modified reports or forms. In-case of split databases, changes in the frontend can be made easily, without any kind of disruption. All you need to do is to link the Access frontend to the backend and test your program. However, things are not always going to be this easy, but it is easier to test new interface objects, while using a split database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Increase the Efficiency of the Bottleneck Step\n",
    "How you increase efficiency in your particular situation will depend on the nature of the process and the available resources, but here are some ideas:\n",
    "\n",
    "Firstly, ensure that whatever you feed into the bottleneck is defect-free. This avoids wasting time on material that will be discarded, or on having to repeat the step.\n",
    "\n",
    "Assign your most skilled team members to the bottleneck. They'll likely be the most productive, too.\n",
    "\n",
    "Or find ways to add capacity in the bottleneck. As the saying goes, many hands make light work! (Hiring extra developers for the season might be the best option in our scenario, if budgets allow.)\n",
    "\n",
    "Finally, automate the step, where possible, to increase the speed of work\n",
    "\n",
    "2. Decrease Input Into the Bottleneck Step\n",
    "Decreasing input is an appropriate response if one part of a process has the potential to produce more output than you ultimately need.\n",
    "\n",
    "In our example, the designers might be creating more adverts than they actually need, but they pass them all on to the development team, just in case. Clearly, this would put unnecessary strain on the developers.\n",
    "\n",
    "An alternative way to decrease input may be to reallocate tasks to where there is more capacity.\n",
    "\n",
    "For example, the designers or marketing team could cross train to complete some of the work that the developers would usually do. This would decrease input into the developers' workflow, and avoid the bottleneck at this stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFRecord format is a simple format for storing a sequence of binary records.\n",
    "\n",
    "Protocol buffers are a cross-platform, cross-language library for efficient serialization of structured data.\n",
    "\n",
    "Protocol messages are defined by .proto files, these are often the easiest way to understand a message type.\n",
    "\n",
    "The tf.train.Example message (or protobuf) is a flexible message type that represents a {\"string\": value} mapping. It is designed for use with TensorFlow and is used throughout the higher-level APIs such as TFX.\n",
    "\n",
    "This notebook demonstrates how to create, parse, and use the tf.train.Example message, and then serialize, write, and read tf.train.Example messages to and from .tfrecord files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol buffers (Protobufs), like XML and JSON, allow applications, which may be written in different languages and running on different platforms, to exchange data. For example, a sending application written in Go could encode a Go-specific sales order in Protobuf, which a receiver written in Java then could decode to get a Java-specific representation of the received order. Here is a sketch of the architecture over a network connection:\n",
    "\n",
    "Go sales order--->Pbuf-encode--->network--->Pbuf-decode--->Java sales order\n",
    "Protobuf encoding, in contrast to its XML and JSON counterparts, is binary rather than text, which can complicate debugging. However, as the code examples in this article confirm, the Protobuf encoding is significantly more efficient in size than either XML or JSON encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. When using TFRecords, when would you want to activate compression? Why not do it systematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPUs have eight cores which act as eight independent workers. We can get data to each core more efficiently by splitting the dataset into multiple files or shards. This way, each core can grab an independent part of the data as it needs.\n",
    "\n",
    "The most convenient kind of file to use for sharding in TensorFlow is a TFRecord. A TFRecord is a binary file that contains sequences of byte-strings. Data needs to be serialized (encoded as a byte-string) before being written into a TFRecord.\n",
    "\n",
    "The most convenient way of serializing data in TensorFlow is to wrap the data with tf.Example. This is a record format based on Google's protobufs but designed for TensorFlow. It's more or less like a dict with some type annotations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
